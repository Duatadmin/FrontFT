# ğŸ™ï¸ VoiceModule â€” Real-Time Audio Streaming with Auto Activation

> A plug-and-play voice input module for web applications using Web Audio, AudioWorklet, and WebSocket. Supports both **push-to-talk** and **walkie-talkie (voice-activated)** modes with real-time transcription via an ASR backend.

---

## ğŸš€ Features

- âœ… Audio streaming from microphone using `AudioWorklet`
- âœ… Converts Float32 to 16-bit PCM in-browser (compatible with Deepgram / ASR APIs)
- âœ… Real-time bi-directional WebSocket connection
- âœ… Dual input modes:
  - **Push-to-Talk** â€” manual press and hold
  - **Walkie-Talkie** â€” auto-activation based on voice level
- âœ… Lightweight, event-based API (`onTranscript`, `onStart`, `onStop`)
- âœ… Easy to embed into any JS frontend
- âœ… Fault-tolerant WebSocket client with auto-reconnect (optional)
- âœ… Modular structure, extensible with TTS, silence detection, or UI overlay

---

## ğŸ“¦ Folder Structure

voice-module/
â”œâ”€â”€ index.js # Main public API (VoiceModule class)
â”œâ”€â”€ config/constants.js # Static config (thresholds, URLs, sample rate)
â”œâ”€â”€ core/
â”‚ â”œâ”€â”€ voice-core.js # Internal logic: audio â†’ websocket â†’ transcript
â”‚ â”œâ”€â”€ session-state.js # FSM: idle â†’ listening â†’ streaming â†’ stopped
â”‚ â””â”€â”€ event-bus.js # Event routing for public callbacks
â”œâ”€â”€ audio/
â”‚ â”œâ”€â”€ media-manager.js # Microphone permission + stream acquisition
â”‚ â”œâ”€â”€ pcm-worklet-node.js # AudioWorkletNode bridge
â”‚ â””â”€â”€ worklet/pcm-processor.js # AudioWorkletProcessor (Float32 â†’ Int16 PCM)
â”œâ”€â”€ streaming/
â”‚ â””â”€â”€ ws-client.js # WebSocket wrapper (binary out, JSON in)
â”œâ”€â”€ modes/
â”‚ â”œâ”€â”€ mode-push.js # Manual (button-press) microphone control
â”‚ â””â”€â”€ mode-walkie.js # Voice-activated logic using RMS threshold
â””â”€â”€ styles/ # Optional UI styles (for microphone indicators)

yaml
ĞšĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ

---

## ğŸ§  How It Works

1. Microphone input is captured at `16kHz` using `AudioWorklet`
2. The stream is converted to 16-bit PCM chunks
3. Chunks are streamed over WebSocket to `/v1/asr/ws`
4. The backend returns partial/final transcriptions as JSON
5. Events are propagated via `onTranscript`, `onStart`, etc.

---

## ğŸ›  Installation

Install from your project root:

```bash
# Clone or copy into your frontend project
git clone https://github.com/your-org/voice-module.git
Or convert to an ES module and publish privately if needed.

ğŸ§ª Usage Example
js
ĞšĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
import { VoiceModule } from './voice-module';

const voice = new VoiceModule({
  onTranscript: ({ text, is_final }) => {
    console.log('ğŸ—£ï¸', text);
    if (is_final) appendToChat(text);
  },
  onStart: () => console.log('ğŸ™ï¸ Listening...'),
  onStop: () => console.log('ğŸ›‘ Stopped.'),
  wsUrl: 'wss://yourdomain.com/v1/asr/ws',
});

await voice.setMode('walkie');  // or 'push'
voice.start();  // you can switch modes later with voice.setMode()
ğŸ•¹ API Reference
new VoiceModule(options)
Options:

Name	Type	Description
onTranscript	(data: { text: string, is_final: boolean }) => void	Called when a transcript is received
onStart	() => void	Called when microphone is activated
onStop	() => void	Called when microphone is deactivated
wsUrl	string	WebSocket endpoint for ASR
mode	`'push'	'walkie'`

voice.start()
Starts the microphone and WebSocket stream according to selected mode.

voice.stop()
Stops the audio stream and closes WebSocket.

voice.setMode(mode: 'push' | 'walkie')
Switches between manual and voice-activated input.
You can call this at runtime to seamlessly change modes; any active
recording will be stopped automatically and the new mode will take effect.

voice.destroy()
Cleans up all resources. Call this when unmounting component/page.

ğŸ§± Push vs Walkie Mode Behavior
Feature	push	walkie
Mic start	On voice.start()	Auto-start on detected speech
Mic stop	On voice.stop()	Auto-stop on silence
UI interaction	Button required	Hands-free
Suitable for...	Button-controlled UIs	Continuous listening / voice-only UX

ğŸŒ Expected WebSocket Protocol
Binary PCM chunks (16-bit signed mono @ 16kHz) sent by client

JSON responses expected from server:

json
ĞšĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
{ "text": "your transcription", "is_final": true }
ğŸ”’ Browser Requirements
Chrome, Edge, Firefox, Safari (latest)

getUserMedia and AudioWorklet support

HTTPS required (except on localhost)

ğŸ“Œ Roadmap / Extensions (Optional)
âœ… Voice activity detection using RMS

ğŸ”œ Silence timer to stop session

ğŸ”œ In-browser TTS playback for AI responses

ğŸ”œ WebSocket reconnect with exponential backoff

ğŸ”œ Native support for Whisper or Vosk as ASR backends